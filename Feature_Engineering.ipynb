{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Engineering"
      ],
      "metadata": {
        "id": "w0Egbq-d6Jsm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter?\n",
        "\n",
        "ANS-A parameter is a variable used to pass information into a function, method, or mathematical expression. In mathematics, parameters define the behavior or shape of a system, such as the slope and intercept in the equation of a line $y = mx + b$. In programming, parameters are placeholders in a function’s definition that allow the function to accept input values. When the function is called, actual values—known as arguments—are provided and assigned to these parameters so the function can work with them. In this way, parameters make functions flexible and reusable, since the same function can produce different results depending on the arguments passed to it.\n"
      ],
      "metadata": {
        "id": "abBlseUa6QFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is correlation?.What does negative correlation mean?\n",
        "\n",
        "ANS-Correlation is a statistical measure that describes the strength and direction of the relationship between two variables. When two variables are correlated, changes in one variable are associated with changes in the other. If both variables increase or decrease together, the correlation is **positive**; if one increases while the other decreases, the correlation is **negative**. When there is no consistent pattern in how the variables move together, the correlation is said to be **zero or weak**. Correlation is often measured using the **correlation coefficient** (commonly Pearson’s $r$), which ranges from $-1$ to $+1$. A value close to $+1$ indicates a strong positive relationship, a value close to $-1$ indicates a strong negative relationship, and a value near $0$ suggests little to no relationship. However, it is important to note that correlation shows association, not causation—just because two variables are correlated does not mean one causes the other.\n",
        "\n",
        "Negative correlation means that as one variable increases, the other decreases, and vice versa. In other words, the two variables move in opposite directions. For example, if the number of hours you spend watching TV increases, the number of hours you have left for studying might decrease—this would be a negative correlation. In statistics, a negative correlation is represented by a correlation coefficient less than 0, with values closer to $-1$ showing a stronger negative relationship. However, just like any correlation, a negative correlation does not imply that one variable directly causes the change in the other—it only shows that they are related in an opposite way.\n",
        "\n"
      ],
      "metadata": {
        "id": "rFGdHk1F6iK6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "ANS-**Machine Learning (ML)** is a branch of artificial intelligence (AI) that focuses on developing algorithms and models that allow computers to learn patterns from data and make decisions or predictions without being explicitly programmed. Instead of following fixed rules, machine learning systems improve their performance as they are exposed to more data over time.\n",
        "\n",
        "The **main components of Machine Learning** are:\n",
        "\n",
        "1. **Data** – The foundation of machine learning; large amounts of relevant and high-quality data are required for training and testing models.\n",
        "2. **Features** – The measurable properties or characteristics extracted from raw data that help the model understand patterns (e.g., height and weight in predicting health outcomes).\n",
        "3. **Model** – The mathematical or computational structure that learns from data and represents the relationship between input (features) and output.\n",
        "4. **Algorithm** – The method or process used to train the model by finding patterns in data (e.g., linear regression, decision trees, neural networks).\n",
        "5. **Training** – The process of feeding data into the model so it can adjust its parameters and learn the relationships between inputs and outputs.\n",
        "6. **Evaluation** – Assessing the performance of the model using metrics (like accuracy, precision, recall, etc.) and test data to ensure it generalizes well.\n",
        "7. **Prediction/Inference** – The final step where the trained model is used to make predictions or decisions on new, unseen data.\n",
        "\n"
      ],
      "metadata": {
        "id": "m_Viz33n6qZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "ANS-The **loss value** measures how far a machine learning model’s predictions are from the actual target values. It acts as a score that tells us how well the model is performing: a **low loss value** means the model’s predictions are close to the true results, while a **high loss value** means the predictions are inaccurate. During training, the model adjusts its parameters to minimize this loss, improving its accuracy over time. By monitoring the loss on both the **training data** and the **validation data**, we can also detect problems like **overfitting** (when the model memorizes training data but performs poorly on new data). In short, the loss value is a key indicator of model quality, showing how well the model learns and generalizes to unseen data.\n"
      ],
      "metadata": {
        "id": "afOV0AoF69X-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are continuous and categorical variables?\n",
        "\n",
        "ANS-**Continuous and categorical variables** are two main types of data used in statistics and machine learning:\n",
        "\n",
        "* **Continuous variables** are numerical values that can take on an infinite number of possible values within a range. They are measurable and can include decimals or fractions. Examples include height, weight, temperature, and income. For instance, someone’s height could be 170.2 cm, 170.25 cm, and so on.\n",
        "\n",
        "* **Categorical variables** represent distinct groups or categories and usually take on a limited set of values. They describe qualities rather than quantities and are often labels or names. Examples include gender (male, female), colors (red, blue, green), or types of vehicles (car, bike, bus). Some categorical variables are **nominal** (no natural order, like colors) and others are **ordinal** (have a natural order, like education level: high school < bachelor’s < master’s)."
      ],
      "metadata": {
        "id": "UnF2vjTD7EGl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\n",
        "ANS-In Machine Learning, categorical variables need to be converted into numerical form because most algorithms work with numbers, not text labels. The way we handle them depends on whether the categories are **nominal** (no order, like colors) or **ordinal** (have order, like education level). Here are the **common techniques**:\n",
        "\n",
        "1. **Label Encoding**\n",
        "\n",
        "   * Assigns a unique number to each category (e.g., Red = 0, Blue = 1, Green = 2).\n",
        "   * Simple and memory-efficient but can mistakenly imply an order when none exists.\n",
        "\n",
        "2. **One-Hot Encoding**\n",
        "\n",
        "   * Creates a new binary (0/1) column for each category.\n",
        "   * Example: \"Color\" with values Red, Blue, Green becomes three columns: \\[IsRed, IsBlue, IsGreen].\n",
        "   * Useful for nominal data, but can increase dimensionality when categories are many.\n",
        "\n",
        "3. **Ordinal Encoding**\n",
        "\n",
        "   * Converts categories into ordered integers based on hierarchy.\n",
        "   * Example: Education Level → High School = 1, Bachelor’s = 2, Master’s = 3, PhD = 4.\n",
        "   * Works well when there is a meaningful ranking.\n",
        "\n",
        "4. **Frequency or Count Encoding**\n",
        "\n",
        "   * Replaces each category with the frequency of its occurrence in the dataset.\n",
        "   * Example: if \"Car\" appears 50 times, \"Bike\" 30 times, and \"Bus\" 20 times, they are encoded as 50, 30, and 20.\n",
        "\n",
        "5. **Target Encoding (Mean Encoding)**\n",
        "\n",
        "   * Replaces each category with the mean of the target variable for that category.\n",
        "   * Example: if in a dataset predicting purchase (Yes=1, No=0), \"Car\" users buy 70% of the time, then \"Car\" is encoded as 0.7.\n",
        "   * Powerful but prone to overfitting, so often used with cross-validation."
      ],
      "metadata": {
        "id": "fGFQdbPJ7NTo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What do you mean by training and testing a dataset?\n",
        "\n",
        "ANS-**Training and testing a dataset** refers to the process of splitting data into two parts so that a machine learning model can be built and evaluated properly.\n",
        "\n",
        "* **Training dataset**: This is the portion of data used to \"teach\" the model. The model analyzes the input features and learns the patterns or relationships between inputs and the target output. Essentially, this is where the model adjusts its parameters to minimize errors.\n",
        "\n",
        "* **Testing dataset**: After training, the model is evaluated on a separate portion of data (the test set) that it has never seen before. The goal is to check how well the model generalizes to new, unseen data. If the model performs well on the test set, it means it has learned useful patterns rather than just memorizing the training data."
      ],
      "metadata": {
        "id": "Z31G0-MK7ZNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is sklearn.preprocessing?\n",
        "\n",
        "ANS-`sklearn.preprocessing` is a **module in scikit-learn (sklearn)** that provides a collection of functions and classes to **prepare and transform raw data** before feeding it into machine learning models. Since many algorithms work best when the data is properly scaled, encoded, or standardized, preprocessing helps improve model performance and accuracy.\n",
        "\n",
        "Some common tasks in `sklearn.preprocessing` include:\n",
        "\n",
        "1. **Scaling and Normalization**\n",
        "\n",
        "   * `StandardScaler`: standardizes features (mean = 0, standard deviation = 1).\n",
        "   * `MinMaxScaler`: scales features to a fixed range, usually \\[0, 1].\n",
        "   * `Normalizer`: scales individual samples to have unit norm.\n",
        "\n",
        "2. **Encoding Categorical Variables**\n",
        "\n",
        "   * `LabelEncoder`: converts categorical labels into numerical values.\n",
        "   * `OneHotEncoder`: creates binary columns for each category.\n",
        "   * `OrdinalEncoder`: encodes categories with an integer value while preserving order.\n",
        "\n",
        "3. **Generating Features**\n",
        "\n",
        "   * `PolynomialFeatures`: creates interaction and polynomial terms (e.g., $x^2, xy$) from existing features.\n",
        "\n",
        "4. **Imputation (Handling Missing Values)**\n",
        "\n",
        "   * `SimpleImputer`: replaces missing values with mean, median, most frequent value, or a constant."
      ],
      "metadata": {
        "id": "HfXz3i1l7htl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is a Test set?\n",
        "\n",
        "ANS-A **test set** is a portion of a dataset that is kept aside and used only to evaluate the performance of a trained machine learning model. Unlike the **training set**, which the model uses to learn patterns and adjust its parameters, the test set contains data the model has never seen before. The purpose of the test set is to check how well the model can generalize to new, unseen data.\n",
        "\n",
        "If a model performs well on the training data but poorly on the test set, it usually means the model has **overfit**—it memorized the training data instead of learning general patterns. By testing on fresh data, we get a realistic estimate of how the model will perform in real-world situations."
      ],
      "metadata": {
        "id": "B9TtJsCB7tlv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?\n",
        "\n",
        "ANS-In Python, data is commonly split into training and testing sets using the `train_test_split` function from the **scikit-learn** library. The training set is used to teach the model by finding patterns in the data, while the test set is kept aside to evaluate how well the model performs on unseen data. Typically, about 70–80% of the data is assigned to the training set and the remaining 20–30% to the test set. The function allows you to specify the split ratio using the `test_size` parameter and ensures reproducibility with the `random_state` parameter. This process is essential because it prevents the model from simply memorizing the dataset and provides a reliable way to measure its ability to generalize to new data.\n",
        "\n",
        "Approaching a Machine Learning problem involves a structured process to ensure that the solution is both effective and reliable. The first step is to define the problem clearly, including the goal (e.g., predicting sales, classifying emails) and identifying whether it is a supervised, unsupervised, or reinforcement learning task. Next, you collect and understand the data, exploring it through visualization and descriptive statistics to identify patterns, trends, and potential issues such as missing values or outliers. After this, you preprocess the data, which includes cleaning, handling missing values, encoding categorical variables, scaling numerical features, and splitting the dataset into training and testing sets.\n",
        "\n",
        "Once the data is ready, you select appropriate algorithms and models based on the problem type (e.g., regression, classification, clustering) and train the models on the training set. After training, the models are evaluated on unseen test data using relevant metrics such as accuracy, precision, recall, F1-score, or mean squared error. If performance is not satisfactory, you may refine the model through hyperparameter tuning, feature engineering, or by trying different algorithms. Finally, once a reliable model is built, it can be deployed into real-world applications, and continuous monitoring is required to ensure it performs well as new data becomes available.\n"
      ],
      "metadata": {
        "id": "sL4_zs3370wA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "ANS-We perform Exploratory Data Analysis (EDA) before fitting a model because it helps us understand the structure, quality, and patterns in the data, which directly affect model performance. EDA allows us to detect missing values, outliers, and inconsistencies that could mislead the model if left untreated. It also reveals distributions, relationships, and correlations between variables, helping us choose the right preprocessing steps, features, and algorithms. For example, visualizing data might show that some features are highly correlated or irrelevant, guiding us to remove or transform them. Without EDA, we risk feeding poor-quality or misleading data into the model, leading to inaccurate predictions or overfitting. In short, EDA ensures that the dataset is clean, well-understood, and ready, making model training more reliable and effective."
      ],
      "metadata": {
        "id": "zj27tugT8ENN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is correlation?\n",
        "\n",
        "ANS-**Correlation** is a statistical measure that shows the strength and direction of the relationship between two variables. If two variables increase or decrease together, they have a **positive correlation**; if one increases while the other decreases, they have a **negative correlation**; and if changes in one variable do not consistently affect the other, the correlation is **close to zero** (no correlation). Correlation is usually measured with a **correlation coefficient** (like Pearson’s $r$), which ranges from $-1$ to $+1$. A value close to $+1$ means a strong positive relationship, close to $-1$ means a strong negative relationship, and close to $0$ means little to no relationship. However, correlation only indicates **association**, not **causation**—two variables may be correlated without one directly causing the other.\n"
      ],
      "metadata": {
        "id": "wpBKvX6j8pQd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What does negative correlation mean?\n",
        "\n",
        "ANS-**Negative correlation** means that two variables move in opposite directions: as one variable increases, the other decreases, and vice versa. For example, if the number of hours a student spends watching TV goes up, their study time may go down—showing a negative correlation. In statistics, this is represented by a correlation coefficient less than 0, with values closer to $-1$ indicating a stronger negative relationship. However, just like all correlation, a negative correlation shows only an association, not a cause-and-effect relationship.\n"
      ],
      "metadata": {
        "id": "tj4yzZ3I8xki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. How can you find correlation between variables in Python?\n",
        "\n",
        "ANS-In Python, correlation between variables can be calculated using the **pandas** library, which provides the `.corr()` method to compute pairwise correlation coefficients between columns in a DataFrame. For example, by creating a DataFrame with features like hours studied, exam scores, and TV hours, calling `df.corr()` returns a correlation matrix showing how strongly each variable is related to the others. Positive values indicate a positive correlation, negative values indicate a negative correlation, and values close to zero indicate little or no correlation. To better visualize these relationships, libraries like **seaborn** can be used to create a heatmap of the correlation matrix, making it easy to identify strong positive or negative relationships among variables. This process helps in understanding patterns in the data and guides feature selection for machine learning models.\n"
      ],
      "metadata": {
        "id": "AhP85r-n84-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "ANS-**Causation** refers to a relationship where one event or variable **directly causes** a change in another. In other words, a causal relationship implies that a change in one variable **produces an effect** on another.\n",
        "\n",
        "The difference between **correlation** and **causation** is important:\n",
        "\n",
        "* **Correlation** means two variables are related or move together, but it does **not** imply that one causes the other.\n",
        "* **Causation** means that changes in one variable **directly bring about changes** in the other.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "* **Correlation:** Ice cream sales and drowning incidents are often positively correlated—they both increase in summer. However, buying ice cream does not cause drowning. Here, the correlation exists because both are influenced by a third factor: **hot weather**.\n",
        "* **Causation:** Smoking and lung cancer have a causal relationship. Scientific studies show that smoking directly increases the risk of developing lung cancer.\n"
      ],
      "metadata": {
        "id": "0AabSEco9EYI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "ANS-An **optimizer** in machine learning and deep learning is an algorithm or method used to **adjust the parameters of a model** (like weights in neural networks) in order to **minimize the loss function**. The goal is to make the model’s predictions as close as possible to the actual target values by finding the optimal set of parameters.\n",
        "\n",
        "Optimizers determine **how the model learns** from the data by controlling the step size and direction in which the model’s parameters are updated during training.\n",
        "\n",
        "### Common Types of Optimizers:\n",
        "\n",
        "1. **Gradient Descent (GD)**\n",
        "\n",
        "   * **Description:** Updates model parameters in the direction of the negative gradient of the loss function.\n",
        "   * **Example:** For linear regression, gradient descent adjusts the slope and intercept to minimize mean squared error.\n",
        "   * **Variants:**\n",
        "\n",
        "     * **Batch Gradient Descent:** Uses the entire training dataset to compute gradients. Accurate but slow for large datasets.\n",
        "     * **Stochastic Gradient Descent (SGD):** Uses one training example at a time. Faster but can be noisy.\n",
        "     * **Mini-batch Gradient Descent:** Uses a small subset of data. Balances speed and accuracy.\n",
        "\n",
        "2. **Momentum**\n",
        "\n",
        "   * **Description:** Accelerates gradient descent by considering past gradients to smooth updates and avoid oscillations.\n",
        "   * **Example:** Helps a neural network converge faster when training on complex data.\n",
        "\n",
        "3. **AdaGrad (Adaptive Gradient Algorithm)**\n",
        "\n",
        "   * **Description:** Adjusts the learning rate for each parameter individually based on past gradients. Parameters with larger gradients get smaller updates.\n",
        "   * **Example:** Useful for sparse data, like text classification, where some features appear more often than others.\n",
        "\n",
        "4. **RMSProp (Root Mean Square Propagation)**\n",
        "\n",
        "   * **Description:** Similar to AdaGrad but uses a moving average of squared gradients to prevent the learning rate from shrinking too much.\n",
        "   * **Example:** Commonly used in recurrent neural networks (RNNs) for sequence prediction tasks.\n",
        "\n",
        "5. **Adam (Adaptive Moment Estimation)**\n",
        "\n",
        "   * **Description:** Combines momentum and RMSProp by using both the moving average of gradients and squared gradients. Efficient and widely used.\n",
        "   * **Example:** Works well for deep learning models like convolutional neural networks (CNNs) for image classification."
      ],
      "metadata": {
        "id": "NPT8OiNe9RXJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What is sklearn.linear_model ?\n",
        "\n",
        "ans-`sklearn.linear_model` is a module in the **scikit-learn (sklearn)** library that provides classes and functions for **linear models** in machine learning. Linear models are algorithms that assume a **linear relationship** between input features (independent variables) and the target variable (dependent variable). This module allows you to perform both **regression** and **classification** tasks using linear approaches.\n",
        "\n",
        "### Key Features of `sklearn.linear_model`:\n",
        "\n",
        "1. **Linear Regression** (`LinearRegression`)\n",
        "\n",
        "   * Predicts a continuous target variable based on a linear combination of input features.\n",
        "   * Example: Predicting house prices based on size, location, and number of rooms.\n",
        "\n",
        "2. **Ridge Regression** (`Ridge`)\n",
        "\n",
        "   * A type of linear regression with **L2 regularization** to prevent overfitting.\n",
        "   * Example: When features are highly correlated, Ridge regression can stabilize the model.\n",
        "\n",
        "3. **Lasso Regression** (`Lasso`)\n",
        "\n",
        "   * Linear regression with **L1 regularization**, which can shrink some coefficients to zero, effectively performing **feature selection**.\n",
        "\n",
        "4. **Logistic Regression** (`LogisticRegression`)\n",
        "\n",
        "   * Used for **binary or multiclass classification**, modeling the probability of categorical outcomes.\n",
        "   * Example: Predicting whether an email is spam (1) or not spam (0).\n",
        "\n",
        "5. **ElasticNet** (`ElasticNet`)\n",
        "\n",
        "   * Combines both L1 and L2 regularization to balance feature selection and coefficient shrinkage.\n",
        "\n",
        "6. **SGDRegressor and SGDClassifier**\n",
        "\n",
        "   * Implement linear models optimized with **stochastic gradient descent**, suitable for large-scale datasets.\n"
      ],
      "metadata": {
        "id": "myi3zF5PGa4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What does model.fit() do? What arguments must be given?\n",
        "ANS-In machine learning using scikit-learn, the method **`model.fit()`** is used to train a model on a given dataset. When you call `fit()`, the model learns patterns from the input data and adjusts its internal parameters, such as weights or coefficients, to minimize the error between its predictions and the actual target values. The method requires two main arguments: **`X`**, which represents the input features as a 2D array or DataFrame, and **`y`**, which represents the target variable as a 1D or 2D array. Once the model is fitted, it has “learned” the relationships between the features and the target, allowing you to make predictions on new data using `model.predict()`. The `fit()` method does not return a new object; it updates the existing model in place, and it is essential that the number of samples in `X` and `y` match so the training process works correctly.\n"
      ],
      "metadata": {
        "id": "WQ8emEM9HgPr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What does model.predict() do? What arguments must be given?\n",
        "\n",
        "ANS-In machine learning using scikit-learn, **`model.predict()`** is used to generate predictions from a trained model. After a model has been trained with `model.fit()`, it has learned the relationship between the input features and the target variable, and `predict()` applies this learned relationship to new, unseen data to produce predicted values or class labels. The method requires a single argument, **`X`**, which represents the input features for which predictions are desired. `X` must have the same number of features (columns) as the data used to train the model and is usually provided as a 2D array or DataFrame. The output of `predict()` depends on the type of model: for regression models, it returns predicted numerical values, while for classification models, it returns predicted class labels. This method does not require the true target values; it simply produces predictions based on the model’s learned parameters, allowing the model to be applied to new data in practical applications.\n"
      ],
      "metadata": {
        "id": "oMAZ2cj-HtXu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What are continuous and categorical variables?\n",
        "\n",
        "ANS-**Continuous and categorical variables** are two main types of data used in statistics and machine learning. **Continuous variables** are numerical and can take on an infinite number of values within a range. They are measurable and often include decimals or fractions, such as height, weight, temperature, or income. For example, a person’s height could be 170.2 cm or 170.25 cm, and both values are valid. **Categorical variables**, on the other hand, represent distinct groups or categories and usually take on a limited set of values. They describe qualities or labels rather than quantities, such as gender (male, female), colors (red, blue, green), or types of vehicles (car, bike, bus). Categorical variables can be **nominal**, meaning there is no natural order (e.g., colors), or **ordinal**, meaning there is a meaningful order (e.g., education level: high school < bachelor’s < master’s). In short, continuous variables measure quantities on a scale, while categorical variables classify data into distinct groups.\n"
      ],
      "metadata": {
        "id": "hGR21zmGIBUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "ANS-**Feature scaling** is a data preprocessing technique in machine learning that **standardizes or normalizes the range of independent variables (features)** so that they have a similar scale. Many machine learning algorithms, such as gradient descent-based models, K-Nearest Neighbors, and Support Vector Machines, are sensitive to the scale of input features. If features have vastly different ranges, the model may give disproportionate importance to variables with larger magnitudes, leading to slower convergence or biased results.\n",
        "\n",
        "Feature scaling helps by **bringing all features to a comparable scale**, which improves model performance, speeds up training, and ensures that all features contribute equally. Common techniques include **Min-Max Scaling**, which transforms features to a fixed range (usually 0 to 1), and **Standardization (Z-score normalization)**, which rescales features to have a mean of 0 and a standard deviation of 1. By applying feature scaling, models can learn more effectively, converge faster, and produce more accurate predictions.\n"
      ],
      "metadata": {
        "id": "UMAbB32BIMVz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. How do we perform scaling in Python?\n",
        "\n",
        "ANS-In Python, feature scaling is typically performed using the **`sklearn.preprocessing`** module, which provides tools like `StandardScaler` and `MinMaxScaler` to standardize or normalize numerical features. For example, `StandardScaler` transforms features to have a mean of 0 and a standard deviation of 1, while `MinMaxScaler` scales features to a fixed range, usually between 0 and 1. The process involves first fitting the scaler to the training data using `fit()` to compute scaling parameters, and then applying `transform()` to scale the data; the combined method `fit_transform()` can be used for convenience. It is important to fit the scaler on the training set and then transform the test set using the same parameters to prevent data leakage. By performing feature scaling, all features are brought to a comparable range, which improves model performance, ensures faster convergence in gradient-based algorithms, and prevents features with larger magnitudes from dominating the learning process.\n"
      ],
      "metadata": {
        "id": "wPoNaCLCIUgH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is sklearn.preprocessing?\n",
        "\n",
        "ANS-`sklearn.preprocessing` is a module in the **scikit-learn** library that provides a variety of tools to **prepare and transform raw data** before it is used to train machine learning models. Since most machine learning algorithms perform better when data is clean, standardized, or properly encoded, this module helps in **scaling, normalizing, and encoding features** so that the model can learn effectively.\n",
        "\n",
        "It includes functionality for tasks such as **feature scaling**, like `StandardScaler` (standardization) and `MinMaxScaler` (normalization), **encoding categorical variables**, like `LabelEncoder`, `OneHotEncoder`, and `OrdinalEncoder`, as well as **handling missing values** with classes like `SimpleImputer`. It also provides tools for **generating new features**, such as `PolynomialFeatures`, to create interaction or polynomial terms from existing features."
      ],
      "metadata": {
        "id": "3GkRyeh3IfJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "ANS-In Python, data is commonly split into training and testing sets using the **`train_test_split`** function from the **`sklearn.model_selection`** module. This ensures that part of the dataset is used to train the model while another part is reserved to evaluate its performance on unseen data. The function takes the input features (`X`) and target variable (`y`) as arguments and allows you to specify the proportion of data for testing using the `test_size` parameter, typically set between 0.2 and 0.3. You can also set the `random_state` parameter to ensure that the split is reproducible. For example, if `X` contains the features and `y` contains the target, calling `train_test_split(X, y, test_size=0.3, random_state=42)` will return four datasets: `X_train`, `X_test`, `y_train`, and `y_test`, which can then be used for model training and evaluation. This process helps prevent overfitting and provides a reliable estimate of how the model will perform on new, unseen data.\n"
      ],
      "metadata": {
        "id": "aj1yDKZkIouF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Explain data encoding?\n",
        "\n",
        "ANS-**Data encoding** is the process of transforming categorical data into a numerical format so that machine learning algorithms can process it. Most machine learning models require numeric inputs, so textual or categorical variables, such as gender, color, or product type, must be converted into numbers.\n",
        "\n",
        "There are several common techniques for data encoding:\n",
        "\n",
        "1. **Label Encoding** – Assigns a unique integer to each category. For example, `Red = 0, Blue = 1, Green = 2`. It is simple but can imply an order that may not exist.\n",
        "\n",
        "2. **One-Hot Encoding** – Creates a separate binary column for each category, with 1 indicating presence and 0 indicating absence. For example, a “Color” column with Red, Blue, and Green becomes three columns: `IsRed`, `IsBlue`, `IsGreen`.\n",
        "\n",
        "3. **Ordinal Encoding** – Assigns integers to categories based on a meaningful order. For example, education levels: `High School = 1, Bachelor’s = 2, Master’s = 3`.\n",
        "\n",
        "4. **Frequency or Target Encoding** – Replaces categories with the frequency of occurrence or the mean of the target variable for that category, useful in certain predictive models.\n",
        "\n",
        "Encoding categorical data correctly ensures that models interpret the information properly, improves training efficiency, and often enhances prediction accuracy.\n"
      ],
      "metadata": {
        "id": "Ttme5kByIv89"
      }
    }
  ]
}