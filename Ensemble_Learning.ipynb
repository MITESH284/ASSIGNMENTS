{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.\n",
        "\n",
        "Answer:Ensemble learning in machine learning is a technique that combines multiple individual models, often called base learners or weak learners, to create a more powerful and accurate predictive model. The key idea behind ensemble learning is that a group of models working together can achieve better performance than any single model alone. This approach leverages the diversity among different models to reduce errors, such as bias and variance, and to improve generalization on unseen data. By aggregating the predictions of several models through methods like bagging, boosting, or stacking, ensemble learning enhances robustness and minimizes the risk of overfitting. Essentially, it follows the principle that “the wisdom of the crowd” can lead to more reliable and precise predictions in machine learning tasks.\n"
      ],
      "metadata": {
        "id": "MKjCDnwvQf4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between Bagging and Boosting?\n",
        "\n",
        "Answer:Bagging and boosting are both popular ensemble learning techniques used to improve the performance and accuracy of machine learning models, but they differ in how they build and combine multiple models. Bagging, which stands for Bootstrap Aggregating, trains multiple independent models in parallel on different random subsets of the training data created through bootstrapping. Each model votes equally when making the final prediction, which helps reduce variance and prevent overfitting. Random Forest is a common example of a bagging-based algorithm. On the other hand, boosting builds models sequentially, where each new model focuses on correcting the errors made by the previous ones. It assigns higher weights to misclassified samples so that subsequent models can pay more attention to difficult cases. The final prediction is obtained by combining all models’ weighted outputs, resulting in improved accuracy and reduced bias. Examples of boosting algorithms include AdaBoost, Gradient Boosting, and XGBoost. In short, bagging reduces variance by training models independently in parallel, while boosting reduces bias by training models sequentially with a focus on previous errors.\n",
        "\n"
      ],
      "metadata": {
        "id": "qMpPj-GrQsqk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?\n",
        "\n",
        "Answer:Bootstrap sampling is a statistical technique used to create multiple random samples from a given dataset by sampling **with replacement**. This means that each new dataset, called a bootstrap sample, is formed by randomly selecting data points from the original dataset such that some observations may appear multiple times, while others may not appear at all. In the context of bagging methods like Random Forest, bootstrap sampling plays a crucial role in ensuring diversity among the base models. Each decision tree in a Random Forest is trained on a different bootstrap sample of the data, which introduces variability and reduces the likelihood that all trees will make the same errors. This diversity among individual trees helps in decreasing overfitting and improves the overall generalization ability of the ensemble model. When the predictions of all trees are averaged (for regression) or combined through majority voting (for classification), the result is a more stable and accurate model that performs better than any single tree trained on the full dataset.\n"
      ],
      "metadata": {
        "id": "uHWyv6T_Qylz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        "\n",
        "Answer:Out-of-Bag (OOB) samples are the data points from the original dataset that are not included in a particular bootstrap sample during the training of an ensemble model like a Random Forest. Since bootstrap sampling is done with replacement, roughly one-third of the data is left out of each sample on average. These excluded data points serve as the OOB samples for that specific model. The OOB score is an internal validation technique that uses these samples to evaluate the performance of the ensemble model without the need for a separate validation or test dataset. After each base model (such as a decision tree) is trained on its bootstrap sample, it is tested on its corresponding OOB samples, and the predictions are compared to the actual values to calculate the error or accuracy. The OOB score is then computed as the average performance across all models using their respective OOB samples. This provides an unbiased estimate of the model’s generalization performance, helping assess accuracy and detect overfitting efficiently.\n"
      ],
      "metadata": {
        "id": "1LNidaaUQ6WW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n",
        "\n",
        "Answer:Feature importance analysis in a single Decision Tree and a Random Forest differs primarily in how the importance scores are computed and how reliable they are. In a single Decision Tree, feature importance is determined based on how much each feature contributes to reducing impurity (such as Gini impurity or entropy) when it is used for splitting nodes. The more a feature helps in making accurate splits, the higher its importance score. However, a single tree can be unstable and highly sensitive to small changes in the data, which may lead to biased or inconsistent feature importance results.\n",
        "\n",
        "In contrast, a Random Forest, which is an ensemble of many decision trees trained on different bootstrap samples and subsets of features, computes feature importance by averaging the importance scores of each feature across all trees in the forest. This aggregation process reduces variance and produces a more robust and reliable measure of feature relevance. Additionally, since Random Forests consider multiple random subsets of features, they help prevent bias toward features with many levels or higher cardinality. As a result, feature importance analysis in a Random Forest provides a more stable, accurate, and generalizable understanding of which features have the greatest influence on the model’s predictions compared to a single Decision Tree.\n"
      ],
      "metadata": {
        "id": "8SrlRffuRAra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:\n",
        "\n",
        "● Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "\n",
        "● Train a Random Forest Classifier\n",
        "\n",
        "● Print the top 5 most important features based on feature importance scores.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "6f7-BQfkRHm5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "icDoWKmNSdCd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "973d803e-f112-4742-8412-3c6a97026cea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "worst area              0.139357\n",
            "worst concave points    0.132225\n",
            "mean concave points     0.107046\n",
            "worst radius            0.082848\n",
            "worst perimeter         0.080850\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "importances = pd.Series(model.feature_importances_, index=data.feature_names)\n",
        "top_features = importances.sort_values(ascending=False).head(5)\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(top_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "\n",
        "● Evaluate its accuracy and compare with a single Decision Tree\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "BqLfL36zRWGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "bagging = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, y_pred_bagging)\n",
        "\n",
        "print(\"Accuracy of Single Decision Tree:\", dt_accuracy)\n",
        "print(\"Accuracy of Bagging Classifier:\", bagging_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbXGPnVZRb8s",
        "outputId": "2b7cacaa-e02c-45ca-94bd-804bda0bb582"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Single Decision Tree: 1.0\n",
            "Accuracy of Bagging Classifier: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "\n",
        "● Train a Random Forest Classifier\n",
        "\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "\n",
        "● Print the best parameters and final accuracy\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "SAeDpPzqRmp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [3, 5, 7, None]\n",
        "}\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Final Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYYM8-kyRmHC",
        "outputId": "07dd261f-963c-47cd-ae92-aebf2ee38c21"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 3, 'n_estimators': 150}\n",
            "Final Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "\n",
        "● Compare their Mean Squared Errors (MSE)\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "qdlPNIkFRza_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "bagging = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=50, random_state=42)\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging.predict(X_test)\n",
        "mse_bagging = mean_squared_error(y_test, y_pred_bagging)\n",
        "\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "print(\"Mean Squared Error (Bagging Regressor):\", mse_bagging)\n",
        "print(\"Mean Squared Error (Random Forest Regressor):\", mse_rf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVI4w-j-R5xs",
        "outputId": "21c66f20-3298-4b9e-c674-c169a5293a87"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (Bagging Regressor): 0.25787382250585034\n",
            "Mean Squared Error (Random Forest Regressor): 0.25650512920799395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "\n",
        "● Choose between Bagging or Boosting\n",
        "\n",
        "● Handle overfitting\n",
        "\n",
        "● Select base models\n",
        "\n",
        "● Evaluate performance using cross-validation\n",
        "\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        "\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:To approach the problem of predicting loan default using ensemble techniques, the process would involve several key steps. First, deciding between Bagging and Boosting depends on the nature of the dataset and the target problem. Bagging, such as Random Forests, is preferable when the goal is to reduce variance and avoid overfitting, especially with high-dimensional or noisy data. Boosting methods, like Gradient Boosting or XGBoost, are more suitable when the dataset has complex patterns and we want to reduce bias by sequentially improving weak learners. In this financial context, Boosting is often favored because it can focus on difficult-to-predict cases, such as customers with borderline default risk.\n",
        "\n",
        "To handle overfitting, careful preprocessing and regularization are essential. This includes cleaning the data, encoding categorical variables, scaling numerical features, removing multicollinearity, and possibly limiting tree depth or the number of estimators in the ensemble. Bagging naturally reduces overfitting through averaging, while boosting may require techniques like learning rate tuning, early stopping, or subsampling.\n",
        "\n",
        "Selecting base models depends on the problem complexity. Decision Trees are the most common choice because they can capture non-linear relationships and interactions between features. In Bagging, multiple decision trees are trained independently; in Boosting, weak decision trees (shallow trees) are trained sequentially to correct errors of prior trees.\n",
        "\n",
        "For performance evaluation, cross-validation is critical. A k-fold cross-validation approach ensures that the model’s predictive performance is tested on multiple subsets of the data, providing an unbiased estimate of its generalization ability. Metrics such as accuracy, precision, recall, F1-score, and AUC-ROC are particularly important in financial risk modeling to assess both correct predictions and false positives/negatives.\n",
        "\n",
        "Ensemble learning improves decision-making in this real-world context by combining multiple models to produce more reliable predictions. For a financial institution, this means better identification of high-risk borrowers, minimizing loan defaults, and optimizing lending decisions. Bagging reduces variance and ensures robust predictions, while Boosting increases predictive accuracy by focusing on challenging cases. The aggregated predictions from an ensemble are less likely to be affected by noise or biases from a single model, enabling the institution to make data-driven, confident, and risk-aware decisions.\n"
      ],
      "metadata": {
        "id": "NlmxrAbDSVD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing, load_breast_cancer\n",
        "\n",
        "\n",
        "data = load_breast_cancer()\n",
        "\n",
        "\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target, name='target')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
        "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "rf_scores = cross_val_score(rf, X_train, y_train, cv=5, scoring='roc_auc')\n",
        "gb_scores = cross_val_score(gb, X_train, y_train, cv=5, scoring='roc_auc')\n",
        "\n",
        "rf.fit(X_train, y_train)\n",
        "gb.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "y_pred_gb = gb.predict(X_test)\n",
        "\n",
        "print(\"Random Forest AUC-ROC (CV):\", rf_scores.mean())\n",
        "print(\"Gradient Boosting AUC-ROC (CV):\", gb_scores.mean())\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(\"Gradient Boosting Accuracy:\", accuracy_score(y_test, y_pred_gb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cH6YWa6ESpVl",
        "outputId": "582e7ac0-4b9f-4ff2-bd86-9189dc0ba17f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest AUC-ROC (CV): 0.9855765423410745\n",
            "Gradient Boosting AUC-ROC (CV): 0.9882818672296505\n",
            "Random Forest Accuracy: 0.9649122807017544\n",
            "Gradient Boosting Accuracy: 0.9590643274853801\n"
          ]
        }
      ]
    }
  ]
}