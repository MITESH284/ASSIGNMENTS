{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is the role of filters and feature maps in Convolutional Neural\n",
        "Network (CNN)?\n",
        "\n",
        "Answer:In a Convolutional Neural Network (CNN), filters and feature maps play an important role in extracting useful information from input data such as images. Filters are small matrices of numbers that slide over the input image and perform a mathematical operation called convolution. Each filter is designed to detect specific features like edges, corners, textures, or patterns. When a filter moves across the image, it looks for these features and responds strongly where the feature is present. The result of applying a filter to the input image is called a feature map. Feature maps show where and how strongly a particular feature appears in the image. As the CNN goes deeper, filters learn more complex features, and feature maps help the network understand the image step by step, leading to accurate recognition or classification.\n"
      ],
      "metadata": {
        "id": "FdsK19cqy8PM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Explain the concepts of padding and stride in CNNs(Convolutional Neural\n",
        "Network). How do they affect the output dimensions of feature maps?\n",
        "\n",
        "Answer:Padding and stride are important concepts in Convolutional Neural Networks that control how the convolution operation is applied to the input image. Padding means adding extra pixels, usually zeros, around the border of the input image before applying the filter. Padding helps preserve the spatial size of the image and prevents loss of important information at the edges. Stride refers to how many pixels the filter moves at a time while sliding over the image. A small stride moves the filter one pixel at a time and produces a larger feature map, while a larger stride skips more pixels and produces a smaller feature map. Together, padding and stride directly affect the output dimensions of feature maps. More padding increases or maintains the feature map size, while a larger stride reduces the size of the feature map by covering the image more quickly.\n"
      ],
      "metadata": {
        "id": "RYKZ0ClSzW0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: Define receptive field in the context of CNNs. Why is it important for deep\n",
        "architectures?\n",
        "\n",
        "Answer:In the context of Convolutional Neural Networks, the receptive field refers to the specific region of the input image that a neuron in a feature map is able to “see” or be influenced by. In early layers of a CNN, the receptive field is small and focuses on simple features like edges or corners. As the network goes deeper, the receptive field becomes larger because each layer builds on the previous one. This is important for deep architectures because a larger receptive field allows the network to capture more global and complex patterns, such as shapes or objects, rather than just small details. By gradually increasing the receptive field, deep CNNs can effectively understand both local features and overall structure in the input data, leading to better performance in tasks like image classification and object detection.\n"
      ],
      "metadata": {
        "id": "3CCTzESVzdk4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: Discuss how filter size and stride influence the number of parameters in a\n",
        "CNN.\n",
        "\n",
        "Answer:In a Convolutional Neural Network, filter size and stride influence how many parameters the network has and how it processes the input data. The filter size directly affects the number of parameters because larger filters contain more values that need to be learned. For example, a 5×5 filter has more parameters than a 3×3 filter, which increases the model complexity and memory usage. Stride, on the other hand, does not change the number of parameters in the filters themselves, but it affects how often the filter is applied across the input. A larger stride reduces the size of the resulting feature map, which can lower the number of computations and reduce parameters in later layers such as fully connected layers. Therefore, filter size increases parameters directly, while stride influences the overall parameter count indirectly by controlling feature map dimensions.\n"
      ],
      "metadata": {
        "id": "056HgWJVzkmX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Compare and contrast different CNN-based architectures like LeNet,\n",
        "AlexNet, and VGG in terms of depth, filter sizes, and performance.\n",
        "\n",
        "Answer:LeNet, AlexNet, and VGG are popular CNN-based architectures that differ in depth, filter sizes, and performance. LeNet is one of the earliest CNN models and has a shallow architecture with only a few layers and small filter sizes. It was mainly designed for simple tasks like handwritten digit recognition and performs well on small datasets. AlexNet is deeper than LeNet and uses more layers with larger filter sizes in the initial layers. It introduced important ideas like ReLU activation and dropout, which significantly improved performance on large-scale image classification tasks. VGG is even deeper than AlexNet and is known for its very uniform design, using many layers with small 3×3 filters. This increased depth allows VGG to learn more complex features, resulting in better accuracy, but it also requires more computation and memory. Overall, as depth increases from LeNet to VGG, performance improves, but so does computational cost.\n"
      ],
      "metadata": {
        "id": "t3SjCZILzquk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Using keras, build and train a simple CNN model on the MNIST dataset\n",
        "from scratch. Include code for module creation, compilation, training, and evaluation.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "_pAbZcVAzwsE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-d680SJwy5h3",
        "outputId": "e9a6498c-faf4-4051-fd4a-b1e71501dab7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 52ms/step - accuracy: 0.8671 - loss: 0.4269\n",
            "Epoch 2/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 53ms/step - accuracy: 0.9827 - loss: 0.0550\n",
            "Epoch 3/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 52ms/step - accuracy: 0.9889 - loss: 0.0357\n",
            "Epoch 4/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 52ms/step - accuracy: 0.9919 - loss: 0.0274\n",
            "Epoch 5/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 50ms/step - accuracy: 0.9935 - loss: 0.0203\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.9869 - loss: 0.0385\n",
            "Test accuracy: 0.9890000224113464\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape((60000, 28, 28, 1)) / 255.0\n",
        "x_test = x_test.reshape((10000, 28, 28, 1)) / 255.0\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=64)\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "\n",
        "print(\"Test accuracy:\", test_accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Load and preprocess the CIFAR-10 dataset using Keras, and create a\n",
        "CNN model to classify RGB images. Show your preprocessing and architecture.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "TVjOPQdn0G2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=64)\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "\n",
        "print(\"Test accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRx0Km4U0Gk6",
        "outputId": "6f4a7ece-a895-4796-c0d2-f00b98e715b8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n",
            "Epoch 1/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 79ms/step - accuracy: 0.3500 - loss: 1.7742\n",
            "Epoch 2/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 80ms/step - accuracy: 0.5539 - loss: 1.2518\n",
            "Epoch 3/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 83ms/step - accuracy: 0.6280 - loss: 1.0619\n",
            "Epoch 4/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 78ms/step - accuracy: 0.6695 - loss: 0.9475\n",
            "Epoch 5/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 80ms/step - accuracy: 0.6903 - loss: 0.8795\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.6860 - loss: 0.8953\n",
            "Test accuracy: 0.682699978351593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Using PyTorch, write a script to define and train a CNN on the MNIST\n",
        "dataset. Include model definition, data loaders, training loop, and accuracy evaluation.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "mqOmfQOB0Sbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 12 * 12, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = CNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    for data, target in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1} completed\")\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        output = model(data)\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(\"Test accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7o-w8eF0VK7",
        "outputId": "22c68bed-c7bd-41a5-ebe6-25429b654433"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 completed\n",
            "Epoch 2 completed\n",
            "Epoch 3 completed\n",
            "Epoch 4 completed\n",
            "Epoch 5 completed\n",
            "Test accuracy: 0.9861\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Given a custom image dataset stored in a local directory, write code using\n",
        "Keras ImageDataGenerator to preprocess and train a CNN model.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "LO_AJZHf0o8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "train_dir = \"dataset/train\"\n",
        "validation_dir = \"dataset/validation\"\n",
        "\n",
        "for directory in [train_dir, validation_dir]:\n",
        "    os.makedirs(os.path.join(directory, 'class_a'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(directory, 'class_b'), exist_ok=True)\n",
        "    for class_name in ['class_a', 'class_b']:\n",
        "        dummy_image_path = os.path.join(directory, class_name, 'dummy_image.png')\n",
        "        if not os.path.exists(dummy_image_path):\n",
        "            dummy_image = Image.fromarray(np.random.randint(0, 255, (150, 150, 3), dtype=np.uint8))\n",
        "            dummy_image.save(dummy_image_path)\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(train_generator.num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    epochs=5,\n",
        "    validation_data=validation_generator\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWjR8LvO0zxU",
        "outputId": "41ec2236-e759-4b17-c315-d6799c86b6ad"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 images belonging to 2 classes.\n",
            "Found 2 images belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.5000 - loss: 0.6924 - val_accuracy: 0.5000 - val_loss: 0.7908\n",
            "Epoch 2/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 393ms/step - accuracy: 0.5000 - loss: 0.7837 - val_accuracy: 0.5000 - val_loss: 3.2905\n",
            "Epoch 3/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 668ms/step - accuracy: 0.5000 - loss: 2.6556 - val_accuracy: 0.5000 - val_loss: 0.8740\n",
            "Epoch 4/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - accuracy: 0.5000 - loss: 0.8278 - val_accuracy: 0.5000 - val_loss: 0.8594\n",
            "Epoch 5/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step - accuracy: 0.5000 - loss: 0.8055 - val_accuracy: 0.5000 - val_loss: 0.7785\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7deb02b902f0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working on a web application for a medical imaging startup. Your\n",
        "task is to build and deploy a CNN model that classifies chest X-ray images into “Normal”\n",
        "and “Pneumonia” categories. Describe your end-to-end approach–from data preparation\n",
        "and model training to deploying the model as a web app using Streamlit.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:To build and deploy a CNN model for classifying chest X-ray images into Normal and Pneumonia categories, the first step is data preparation. The dataset is collected and organized into folders for each class, then images are resized and normalized to make them suitable for training. Data augmentation is applied to improve model generalization. Next, a CNN model is trained using convolutional and pooling layers to learn important features from X-ray images, followed by dense layers for classification. After training, the model is saved. For deployment, Streamlit is used to create a simple web application where users can upload an X-ray image. The uploaded image is preprocessed in the same way as training data and passed to the trained model, which then predicts and displays whether the image is Normal or Pneumonia. This end-to-end approach allows easy access to the model through a web interface for real-world use."
      ],
      "metadata": {
        "id": "PCxNNTQz0-Ki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import streamlit as st\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "train_dir = \"chest_xray/train\"\n",
        "val_dir = \"chest_xray/val\"\n",
        "\n",
        "for directory in [train_dir, val_dir]:\n",
        "    os.makedirs(os.path.join(directory, 'NORMAL'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(directory, 'PNEUMONIA'), exist_ok=True)\n",
        "    for class_name in ['NORMAL', 'PNEUMONIA']:\n",
        "        dummy_image_path = os.path.join(directory, class_name, 'dummy_image.png')\n",
        "        if not os.path.exists(dummy_image_path):\n",
        "            dummy_image = Image.fromarray(np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8))\n",
        "            dummy_image.save(dummy_image_path)\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=15,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_gen = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "val_gen = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_gen, epochs=5, validation_data=val_gen)\n",
        "\n",
        "model.save(\"pneumonia_model.h5\")\n",
        "\n",
        "st.title(\"Chest X-ray Classification\")\n",
        "uploaded_file = st.file_uploader(\"Upload Chest X-ray Image\", type=[\"jpg\", \"png\", \"jpeg\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    image = Image.open(uploaded_file).resize((224, 224))\n",
        "    img_array = np.array(image) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    loaded_model = tf.keras.models.load_model(\"pneumonia_model.h5\")\n",
        "    prediction = loaded_model.predict(img_array)\n",
        "\n",
        "    if prediction[0][0] > 0.5:\n",
        "        st.write(\"Prediction: Pneumonia\")\n",
        "    else:\n",
        "        st.write(\"Prediction: Normal\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyZjvQWy1Q_0",
        "outputId": "ba5546c6-4697-4c24-f9b5-e28bc78fd715"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 images belonging to 2 classes.\n",
            "Found 2 images belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.5000 - loss: 0.6915 - val_accuracy: 0.5000 - val_loss: 3.7004\n",
            "Epoch 2/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 555ms/step - accuracy: 0.5000 - loss: 2.8584 - val_accuracy: 0.5000 - val_loss: 1.0202\n",
            "Epoch 3/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 567ms/step - accuracy: 0.5000 - loss: 0.8993 - val_accuracy: 0.5000 - val_loss: 0.7425\n",
            "Epoch 4/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 629ms/step - accuracy: 0.5000 - loss: 0.7243 - val_accuracy: 0.5000 - val_loss: 0.6935\n",
            "Epoch 5/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.5000 - loss: 0.6930 - val_accuracy: 0.5000 - val_loss: 0.7132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "2025-12-25 05:25:01.978 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-25 05:25:02.262 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2025-12-25 05:25:02.263 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-25 05:25:02.265 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-25 05:25:02.268 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-25 05:25:02.269 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-25 05:25:02.269 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-25 05:25:02.271 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-25 05:25:02.272 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-25 05:25:02.273 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    }
  ]
}