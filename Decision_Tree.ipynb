{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "\n",
        "Answer:A Decision Tree is a type of supervised machine learning algorithm used for both classification and regression tasks, though it is most commonly applied to classification problems. Conceptually, it resembles a tree-like structure where each internal node represents a decision based on the value of a particular feature, each branch represents the outcome of that decision, and each leaf node represents a final class label or decision. The tree works by recursively splitting the dataset into subsets based on feature values that provide the most meaningful separation between classes. In the context of classification, the algorithm evaluates potential splits using metrics like Gini Impurity, Information Gain, or Entropy to determine which feature best divides the data into distinct classes. Once the tree is fully grown or meets a stopping criterion, new instances can be classified by traversing the tree from the root node to a leaf node, following the decisions at each internal node that match the instance’s feature values. This makes decision trees intuitive, interpretable, and effective for many practical classification problems.\n"
      ],
      "metadata": {
        "id": "7giSFe0Do8mM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "\n",
        "Answer:Gini Impurity and Entropy are two commonly used metrics in decision trees that measure the “impurity” or disorder of a dataset at a given node, helping the algorithm decide the best feature to split on. Gini Impurity quantifies the probability of incorrectly classifying a randomly chosen element if it were labeled according to the distribution of class labels in that node. A lower Gini Impurity indicates that the node is more homogeneous, meaning most of its samples belong to a single class. Entropy, derived from information theory, measures the amount of uncertainty or randomness in the class distribution at a node. Higher entropy indicates greater disorder and more mixed classes, while zero entropy corresponds to a perfectly pure node with samples from only one class. When constructing a decision tree, the algorithm evaluates potential splits by calculating the reduction in impurity—known as Information Gain for entropy or Gini Gain for Gini Impurity. The split that maximizes the reduction in impurity is chosen, ensuring that the resulting child nodes are as pure as possible. Therefore, these measures directly impact the structure of the tree: nodes with higher homogeneity are preferred, which helps the tree classify data more accurately and efficiently.\n"
      ],
      "metadata": {
        "id": "bffch4anpLow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        "\n",
        "Answer:Pre-pruning and post-pruning are two techniques used in decision trees to prevent overfitting, which occurs when the tree becomes too complex and models noise in the training data rather than the underlying patterns. Pre-pruning, also known as early stopping, involves halting the growth of the tree during its construction based on specific criteria such as a maximum depth, a minimum number of samples required to split a node, or a threshold for improvement in impurity reduction. The main advantage of pre-pruning is that it reduces the computational cost of building the tree because it stops unnecessary splits early, resulting in a simpler model that is faster to train and easier to interpret. Post-pruning, on the other hand, allows the tree to grow fully and then removes branches that do not provide significant predictive power, often by evaluating the impact of pruning on a validation dataset. This approach typically results in higher predictive accuracy because it considers the overall structure of the fully grown tree before deciding which branches are redundant. A practical advantage of post-pruning is that it can achieve a better balance between bias and variance, often producing more robust and generalizable models compared to pre-pruned trees.\n"
      ],
      "metadata": {
        "id": "CRlg5nrEpSDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "\n",
        "Answer:Information Gain is a key metric used in decision trees to evaluate the effectiveness of a potential split at a given node. It is based on the concept of entropy from information theory and measures the reduction in uncertainty about the class labels after splitting the data according to a particular feature. In other words, Information Gain quantifies how much “information” is obtained by knowing the value of that feature. A higher Information Gain indicates that the split results in child nodes that are more homogeneous, meaning the samples within each node belong predominantly to a single class. This is important because the goal of a decision tree is to create nodes that are as pure as possible, which improves the tree’s ability to classify new instances accurately. During tree construction, the algorithm evaluates all possible splits and chooses the one with the highest Information Gain, ensuring that each decision maximally reduces uncertainty and contributes to a more efficient and effective model. By prioritizing splits that yield the greatest reduction in entropy, Information Gain helps guide the tree toward a structure that captures the underlying patterns in the data rather than noise.\n"
      ],
      "metadata": {
        "id": "cGnkZqAapYKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "\n",
        "Answer:Decision Trees have a wide range of real-world applications due to their simplicity, interpretability, and effectiveness in handling both categorical and numerical data. In healthcare, they are used for diagnosing diseases by analyzing patient symptoms and medical test results. In finance, decision trees assist in credit scoring, fraud detection, and risk assessment by evaluating patterns in customer data. In marketing, they help segment customers, predict purchasing behavior, and optimize targeted campaigns. Other applications include predicting equipment failures in manufacturing, classifying species in biology, and guiding decision-making in business strategy. The main advantages of decision trees include their intuitive, visual representation, which makes them easy to interpret and explain to non-technical stakeholders, their ability to handle both continuous and categorical features without extensive preprocessing, and their relatively fast training and prediction times. However, they also have limitations, such as a tendency to overfit on noisy or small datasets, sensitivity to small changes in the data which can lead to different tree structures, and sometimes lower predictive accuracy compared to ensemble methods like Random Forests or Gradient Boosted Trees. Despite these limitations, decision trees remain a foundational tool in machine learning due to their clarity and versatility.\n"
      ],
      "metadata": {
        "id": "Xq0GgnxHpety"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:\n",
        "\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "\n",
        "● Print the model’s accuracy and feature importances\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "qAYYMusApm9z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdrvdzfPo3Hn",
        "outputId": "b7ec5d21-7afc-4ac6-99d4-4e02c6035a91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.00\n",
            "sepal length (cm): 0.000\n",
            "sepal width (cm): 0.019\n",
            "petal length (cm): 0.893\n",
            "petal width (cm): 0.088\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Decision Tree Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "\n",
        "a fully-grown tree.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "Ipm_6b6ZtBbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "print(f\"Fully-grown Tree Accuracy: {accuracy_full:.2f}\")\n",
        "\n",
        "clf_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_limited.fit(X_train, y_train)\n",
        "y_pred_limited = clf_limited.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "print(f\"Tree with max_depth=3 Accuracy: {accuracy_limited:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lj8cu9qwtHUk",
        "outputId": "571fc223-9c8e-420e-9fda-926a14af805e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fully-grown Tree Accuracy: 1.00\n",
            "Tree with max_depth=3 Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "\n",
        "● Load the Boston Housing Dataset\n",
        "\n",
        "● Train a Decision Tree Regressor\n",
        "\n",
        "● Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "smBy2dQ3tP89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "y_pred = regressor.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "\n",
        "for feature, importance in zip(housing.feature_names, regressor.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.3f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Nu-QQsEtUmo",
        "outputId": "da0f1d7a-6f77-4388-d40a-a8d3a94d92a5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.53\n",
            "MedInc: 0.523\n",
            "HouseAge: 0.052\n",
            "AveRooms: 0.049\n",
            "AveBedrms: 0.025\n",
            "Population: 0.032\n",
            "AveOccup: 0.139\n",
            "Latitude: 0.090\n",
            "Longitude: 0.089\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "\n",
        "● Print the best parameters and the resulting model accuracy\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "T-AqVCv0tZPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 3, 4, 5]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "y_pred = grid_search.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_hAUFzZtfS_",
        "outputId": "fc610d7e-e028-43fa-ecf1-237d4043fbc6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 3, 'min_samples_split': 2}\n",
            "Model Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "\n",
        "Explain the step-by-step process you would follow to:\n",
        "\n",
        "● Handle the missing values\n",
        "\n",
        "● Encode the categorical features\n",
        "\n",
        "● Train a Decision Tree model\n",
        "\n",
        "● Tune its hyperparameters\n",
        "\n",
        "● Evaluate its performance\n",
        "\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "\n",
        "Answer:To predict whether a patient has a certain disease using a dataset with mixed data types and missing values, I would follow a structured, step-by-step approach to ensure data quality, model performance, and actionable business insights. The first step is **handling missing values**. For numerical features, missing values can be imputed using strategies such as the mean, median, or a more advanced approach like K-Nearest Neighbors imputation. For categorical features, missing values can be replaced with the mode or a new category labeled “Unknown.” This ensures that the dataset remains complete and avoids biases caused by simply dropping rows with missing data.\n",
        "\n",
        "Next, I would **encode categorical features** so that the Decision Tree can interpret them. Label encoding can be applied to ordinal categorical features where the order matters, while one-hot encoding is suitable for nominal features with no inherent order. Proper encoding allows the tree to evaluate splits on categorical variables accurately.\n",
        "\n",
        "Once the data is cleaned and encoded, I would **train a Decision Tree model**. This involves splitting the dataset into training and test sets to ensure unbiased evaluation, initializing the Decision Tree classifier, and fitting it on the training data. During training, the algorithm uses impurity measures such as Gini Impurity or Entropy to determine optimal splits and create a tree that separates patients into disease-positive or disease-negative groups.\n",
        "\n",
        "After training, I would **tune the model’s hyperparameters** to optimize performance and prevent overfitting. Key parameters include `max_depth` (to limit the tree size), `min_samples_split` (to control the minimum number of samples required to split a node), and `min_samples_leaf` (to enforce a minimum number of samples at leaf nodes). Techniques like GridSearchCV or RandomizedSearchCV can systematically test combinations of these parameters and select the best set based on cross-validated accuracy or other metrics.\n",
        "\n",
        "Finally, I would **evaluate the model’s performance** using the test set, focusing not only on accuracy but also on metrics relevant to healthcare, such as precision, recall, F1-score, and the area under the ROC curve (AUC). High recall is particularly important in disease prediction to minimize false negatives, ensuring patients with the disease are correctly identified. Feature importance analysis can also provide insights into which factors most strongly predict disease risk.\n",
        "\n",
        "In a real-world healthcare setting, this model offers significant **business value**. It can assist clinicians in early disease detection, prioritize high-risk patients for further testing, reduce unnecessary diagnostic costs, and ultimately improve patient outcomes. Moreover, the insights derived from feature importance can inform preventive health strategies and guide resource allocation in hospitals or insurance companies, making the model both a predictive and strategic tool for healthcare decision-making.\n"
      ],
      "metadata": {
        "id": "iXd0vMsItkUa"
      }
    }
  ]
}